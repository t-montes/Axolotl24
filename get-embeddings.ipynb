{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the embeddings to work with the clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAILABLE_GPU = 0\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= f\"{AVAILABLE_GPU}\" # ALWAYS look the one with 0% usage\n",
    "tf_device=f'/gpu:{AVAILABLE_GPU}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/estudiante/axolotl24/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select the model and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('surprise', 'axolotl.test.surprise')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILE_TO_READ = './data/augmented/axolotl.test.surprise.tsv'\n",
    "EMBEDDING_TYPE = 'examples'\n",
    "PRINT_EACH_ROW = False\n",
    "\n",
    "language = FILE_TO_READ.split('.')[-2]\n",
    "filename = FILE_TO_READ.split('/')[-1].split('.')[0:-1]\n",
    "filename = '.'.join(filename)\n",
    "language, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (language == \"ru\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "    model = AutoModel.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "elif (language == \"fi\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"TurkuNLP/bert-base-finnish-cased-v1\")\n",
    "    model = AutoModel.from_pretrained(\"TurkuNLP/bert-base-finnish-cased-v1\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-german-cased\")\n",
    "    model = AutoModel.from_pretrained(\"google-bert/bert-base-german-cased\")\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1190 entries, 0 to 1189\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   usage_id              1190 non-null   object \n",
      " 1   word                  1190 non-null   object \n",
      " 2   orth                  1190 non-null   object \n",
      " 3   sense_id              622 non-null    object \n",
      " 4   gloss                 1190 non-null   object \n",
      " 5   example               1190 non-null   object \n",
      " 6   indices_target_token  1189 non-null   object \n",
      " 7   date                  1189 non-null   float64\n",
      " 8   period                1189 non-null   object \n",
      "dtypes: float64(1), object(8)\n",
      "memory usage: 83.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(FILE_TO_READ, sep='\\t')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean and fix the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are no \"example\" column, the \"gloss\" will be taken, with a sentence like \"Definition of WORD: GLOSS\" depending on the language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition von word: gloss\n"
     ]
    }
   ],
   "source": [
    "if language == \"ru\":\n",
    "    prompt = \"Определение слова {}: {}\"\n",
    "elif language == \"fi\":\n",
    "    prompt = \"Sanan {} määritelmä: {}\"\n",
    "else:\n",
    "    prompt = \"Definition von {}: {}\"\n",
    "\n",
    "print(prompt.format(\"word\", \"gloss\"))\n",
    "\n",
    "def fill_example(word, gloss, example):\n",
    "    if pd.isna(example):\n",
    "        return prompt.format(word, gloss)\n",
    "    else:\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1190 entries, 0 to 1189\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   usage_id              1190 non-null   object \n",
      " 1   word                  1190 non-null   object \n",
      " 2   orth                  1190 non-null   object \n",
      " 3   sense_id              622 non-null    object \n",
      " 4   gloss                 1190 non-null   object \n",
      " 5   example               1190 non-null   object \n",
      " 6   indices_target_token  1189 non-null   object \n",
      " 7   date                  1189 non-null   float64\n",
      " 8   period                1189 non-null   object \n",
      "dtypes: float64(1), object(8)\n",
      "memory usage: 83.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df['example'] = df.apply(lambda row: fill_example(row['word'],row['gloss'],row['example']), axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nice(input_ids, index, index_end=None, pad_token=None):\n",
    "    if pad_token is not None:\n",
    "        input_ids = [token for token in input_ids if token != pad_token]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    if index_end is None:\n",
    "        tokens[index] = '\\033[94m' + tokens[index] + '\\033[0m'\n",
    "    else:\n",
    "        tokens[index] = '\\033[94m' + tokens[index]\n",
    "        tokens[index_end] = tokens[index_end] + '\\033[0m'\n",
    "    print(' '.join(tokens))\n",
    "\n",
    "def generate_substrings(word):\n",
    "    substrings = []\n",
    "    for i in range(len(word), 0, -1):\n",
    "        substrings.append(word[:i])\n",
    "    return substrings[1:-1]\n",
    "\n",
    "def find_sub_list(sl,l): # not used because some examples have no exact coincidence\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind+1,ind+sll # +1 for the [CLS] token\n",
    "\n",
    "def remove_accents(word):\n",
    "    normalized_word = unicodedata.normalize('NFD', word)\n",
    "    cleaned_word = re.sub(r'[\\u0300-\\u036f]', '', normalized_word)\n",
    "    return cleaned_word\n",
    "\n",
    "def isletter(character):\n",
    "    return re.sub(r'[^а-яА-Яa-zA-ZÀ-ÿёЁ\\u0300-\\u036f-]', '', character) != ''\n",
    "\n",
    "def extract_letters(input_string):\n",
    "    if input_string == \"\":\n",
    "        return \"\"\n",
    "    if not isletter(input_string[0]):\n",
    "        input_string = input_string[1:]\n",
    "    if not isletter(input_string[-1]):\n",
    "        input_string = input_string[:-1]\n",
    "    if language == \"ru\":\n",
    "        return re.sub(r\"[^[\\]а-яА-Яa-zA-ZÀ-ÿёЁ\\u0300-\\u036f'-]\", '', input_string)\n",
    "    else:\n",
    "        return re.sub(r\"[^[\\]а-яА-Яa-zA-ZÀ-ÿёЁ\\u0300-\\u036f'-.]\", '', input_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cases where orth or word have multiple words, replace with the first word only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if len(row['orth'].split())>1:\n",
    "        first_occurrence = row['orth'].split()[0]\n",
    "        df.at[index, 'orth'] = first_occurrence\n",
    "        print(f\"Replaced {row['orth']} with {df.at[index, 'orth']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if len(row['word'].split())>1:\n",
    "        first_occurrence = row['word'].split()[0]\n",
    "        df.at[index, 'word'] = first_occurrence\n",
    "        print(f\"{row['word']} -> {df.at[index, 'word']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the regex is working with all the words. The next cell should have no output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "chars_to_replace = [',', '.', \"'\", \":\", ';', '?', '–', ')', ' ', '[', ']']\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # Apply the extract_letters function to 'word' and 'orth' columns\n",
    "    word = row['word']\n",
    "    orth = row['orth']\n",
    "    \n",
    "    for char in chars_to_replace:\n",
    "        word = word.replace(char, '')\n",
    "    for char in chars_to_replace:\n",
    "        orth = orth.replace(char, '')\n",
    "    \n",
    "    clean_word = extract_letters(word)\n",
    "    clean_orth = extract_letters(orth)\n",
    "    \n",
    "    # Check if the cleaned versions are equal to the original values\n",
    "    if clean_word != word or clean_orth != orth:\n",
    "        print(f\"{index}. {word, clean_word} - {orth, clean_orth}\")\n",
    "        count += 1\n",
    "assert count == 0, \"Must fix the regex to include all the characters of the language\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_containing_target(sentence, target_word):\n",
    "    index = sentence.find(target_word)\n",
    "    if index == -1:\n",
    "        return None\n",
    "    start_index = sentence.rfind(\" \", 0, index) + 1 if index != 0 else 0\n",
    "    end_index = sentence.find(\" \", index + len(target_word)) if sentence.find(\" \", index + len(target_word)) != -1 else len(sentence)\n",
    "    final_char = final_char = \" \" if target_word.endswith(\" \") else \"\"\n",
    "    return sentence[start_index:end_index].split()[0] + final_char\n",
    "\n",
    "def get_search(example, word, orth=None, print_search=False):\n",
    "    # append the words to search in the example, in the desired ORDER\n",
    "    # 1 - the word (with an ending character), and the word itself\n",
    "    search = [f\"{word} \", f\"{word},\", f\"{word}.\", word]\n",
    "\n",
    "    # 2 - the orthographic form (with an ending character), and the orthographic form itself\n",
    "    if orth and orth != word:\n",
    "        search += [f\"{orth} \", f\"{orth},\", f\"{orth}.\", orth]\n",
    "\n",
    "    # 3 - all substrings of the word (i.e. выходить -> ['выходит', 'выходи', 'выход', 'выхо', 'вых', 'вы'])\n",
    "    search += generate_substrings(word)\n",
    "\n",
    "    # 4 - all substrings of the orthographic form\n",
    "    if orth:\n",
    "        search.extend([i for i in generate_substrings(orth) if i not in search])\n",
    "\n",
    "    # 5 - the word without accents (with an ending character), and the word without accents itself\n",
    "    unicoded_word = remove_accents(word)\n",
    "    if unicoded_word != word:\n",
    "        search += [f\"{unicoded_word} \", f\"{unicoded_word},\", f\"{unicoded_word}.\", unicoded_word]\n",
    "    \n",
    "    # 6 - the orthographic form without accents (with an ending character), and the orthographic form without accents itself\n",
    "    if orth and orth != word:\n",
    "        unicoded_orth = remove_accents(orth)\n",
    "        if unicoded_orth != orth:\n",
    "            search += [f\"{unicoded_orth} \", f\"{unicoded_orth},\", f\"{unicoded_orth}.\", unicoded_orth]\n",
    "\n",
    "    # 7 - all substrings of the word without accents\n",
    "    if unicoded_word != word:\n",
    "        search.extend([i for i in generate_substrings(unicoded_word) if i not in search])\n",
    "\n",
    "    # 8 - all substrings of the orthographic form without accents\n",
    "    if orth and orth != word and unicoded_orth != orth:\n",
    "        search.extend([i for i in generate_substrings(unicoded_orth) if i not in search])\n",
    "\n",
    "    if print_search:\n",
    "        print(f\"Searching for: {search}\")\n",
    "\n",
    "    # FIND the first search-string that is within the example, if any (in upper or lowercase)\n",
    "    for s in search:\n",
    "        search_word = find_word_containing_target(example, s)\n",
    "        if search_word:\n",
    "            break\n",
    "        search_word = find_word_containing_target(example.lower(), s.lower())\n",
    "        if search_word:\n",
    "            index = example.lower().find(search_word)\n",
    "            if index == -1:\n",
    "                # this should never happen\n",
    "                raise Exception(f\"Found '{search_word}' in '{example.lower()}', but then not found...\")\n",
    "            else:\n",
    "                search_word = example[index:index + len(search_word)]\n",
    "            break\n",
    "    else:\n",
    "        search_word = \"\"\n",
    "    return extract_letters(search_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('zersetzen', 'zersetzen')\n",
      "('Dynamik', 'Dynamik')\n",
      "('packen', 'packen')\n",
      "('Seminar', 'Seminar')\n",
      "('Ohrwurm', 'Ohrwurm')\n",
      "('überspannen', 'überspannen')\n",
      "('ausspannen', 'ausspannen')\n",
      "('Abgesang', 'Abgesang')\n",
      "('Rezeption', 'Rezeption')\n",
      "('Armenhaus', 'Armenhaus')\n",
      "('abbauen', 'abbauen')\n",
      "('Engpaß', 'Engpaß')\n",
      "('Schmiere', 'Schmiere')\n",
      "('Sensation', 'Sensation')\n",
      "('Knotenpunkt', 'Knotenpunkt')\n",
      "('Mißklang', 'Mißklang')\n",
      "('Fuß', 'Füße')\n",
      "('Eintagsfliege', 'Eintagsfliege')\n",
      "('abdecken', 'abdecken')\n",
      "('Spielball', 'Spielball')\n",
      "('abgebrüht', 'abgebrüht')\n",
      "('artikulieren', 'artikulieren')\n",
      "('Titel', 'Titel')\n",
      "('Manschette', 'Manschette')\n",
      "('packen', 'packen')\n",
      "('Seminar', 'Seminar')\n",
      "('ausspannen', 'ausspannen')\n",
      "('Rezeption', 'Rezeption')\n",
      "('Engpaß', 'Engpaß')\n",
      "('Schmiere', 'Schmiere')\n",
      "('Spielball', 'Spielball')\n",
      "('abgebrüht', 'abgebrüht')\n",
      "('Titel', 'Titel')\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "word = \"\"\n",
    "word_idx = 0\n",
    "print((df.loc[0, \"word\"], df.loc[0, \"orth\"]))\n",
    "\n",
    "starter_char = '\\n' if PRINT_EACH_ROW else ''\n",
    "for index, row in df.iterrows():\n",
    "    if word != \"\" and word != row['word']:\n",
    "        print(f\"{starter_char}{row['word'], row['orth']}\")\n",
    "        word_idx += 1\n",
    "\n",
    "    should_print = PRINT_EACH_ROW\n",
    "    word = row['word']          # target word\n",
    "    orth = row['orth']          # usage of the target word in the example\n",
    "    sense_id = row['sense_id']  # sense of the target word in the example\n",
    "    gloss = row['gloss']        # definition of the target word\n",
    "    example = row['example']    # usage example of the target word\n",
    "\n",
    "    if EMBEDDING_TYPE == \"examples\":\n",
    "        # 1. Get the target word index in the example tokenized\n",
    "        search_word = get_search(example, word, orth)\n",
    "        tokens = tokenizer.tokenize(example)\n",
    "        if search_word == \"\":\n",
    "            if len(example.split()) == 1:\n",
    "                print(f\"{index}. \\033[91mNot found\\033[0m {word} in '{example}' (taking only word in example)\")\n",
    "                target_index, target_index_end = 1, 1\n",
    "            else:\n",
    "                print(f\"{index}. \\033[91mNot found\\033[0m {word} in '{example}' (taking [CLS] token)\")\n",
    "                target_index, target_index_end = 0, 0\n",
    "        else:\n",
    "            search_tokens = tokenizer.tokenize(search_word)\n",
    "            try:\n",
    "                target_index, target_index_end = find_sub_list(search_tokens, tokens)\n",
    "            except:\n",
    "                # this should never happen\n",
    "                raise ValueError(f\"Error unpacking {search_tokens} in {tokens}\")\n",
    "        inputs = tokenizer(example, return_tensors=\"pt\", max_length=512, truncation=True, padding='max_length')\n",
    "    else:\n",
    "        # 1. Get the [CLS] token of the gloss\n",
    "        target_index, target_index_end = 0, 0\n",
    "        if f\"{gloss}\" == \"nan\":\n",
    "            raise ValueError(f\"Couldn't get gloss CLS at index {index}, for word {word}\")\n",
    "        inputs = tokenizer(gloss, return_tensors=\"pt\", max_length=512, truncation=True, padding='max_length')\n",
    "\n",
    "    if should_print:\n",
    "        print_nice(inputs['input_ids'][0], target_index, target_index_end, pad_token=tokenizer.pad_token_id)\n",
    "\n",
    "    # 2. Compute the embedding of the token\n",
    "    with torch.no_grad():\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "    \n",
    "    embedding = outputs.last_hidden_state[0][target_index]\n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1190, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(embeddings) == len(df), \"Embeddings and dataframe have different lengths\"\n",
    "embeddings = torch.stack(embeddings)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_list = embeddings.tolist()\n",
    "\n",
    "json_file_path = f'./embeddings/{EMBEDDING_TYPE}/{filename}.json'\n",
    "\n",
    "# Write the embeddings to a JSON file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(embeddings_list, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
